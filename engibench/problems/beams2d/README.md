# Beams 2D (MBB Beam)

**Lead**: Arthur Drake @arthurdrake1

Beams2D is a structural topology optimization (TO) problem that optimizes a 2D Messerschmitt-Bölkow-Blohm (MBB) beam under bending. The beam is symmetric about the central vertical axis, with a force applied at the top; only the right half is modeled in our case.

The goal is to optimize the distribution of solid material within a rectangular grid of size *nelx* × *nely* (default: 100 × 50) to minimize compliance (or maximize stiffness) while satisfying constraints on material usage and minimum feature size.

Problem conditions include:
- Volume fraction of solid material,
- A minimum feature length constraint,
- The position of the downward force,
- An optional overhang constraint to eliminate structurally unsupported members and promote manufacturability.

Our implementation is based on the well-known 88-line MATLAB code for compliance minimization by Andreassen et al. (2011), adapted to Python.

The dataset includes:
- Optimal beam structures for various problem settings,
- Their corresponding conditions and objective values,
- Optimization histories for advanced usage.

This dataset was generated by systematically sampling the condition space. For this problem, we provide three different datasets corresponding to different pairs of *nelx* and *nely*: 50 × 25, 100 × 50, and 200 × 100.

## Side notes

Here is the script I've used to generate the dataset conditions. Please note that `max_iter = 100` and it is assumed that `nelx = 2*nely`. This yields a total of 14553 samples, or 4851 samples for each of the three image resolutions.

```python
all_params = [
    np.array([25, 50, 100]),                        # nely (nelx = 2*nely)
    np.round(np.linspace(0.15, 0.4, 21), 4),        # volfrac
    np.round(np.linspace(1.5, 4.0, 11), 4),         # rmin
    np.round(np.linspace(0, 1, 21), 4)              # forcedist
]

params = np.array(np.meshgrid(*all_params)).T.reshape(-1, len(all_params))

```

Here is the script I've used to upload the data to HF:

```python
import os
import pickle
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import Dataset, DatasetDict
from huggingface_hub import HfApi
from sklearn.model_selection import train_test_split

output_path = os.path.join(data_dir, "_data.pkl")

# Load the Pickle file
with open(output_path, "rb") as f:
    design_dict = pickle.load(f)

# Hugging Face API instance
api = HfApi()

# Loop through each resolution and create a Hugging Face dataset
for resolution, data in design_dict.items():
    print(f"Processing resolution: {resolution}...")

    # Convert dictionary list to Hugging Face format
    dataset_list = data

    # Split dataset into Train (80%), Val (15%), Test (5%)
    train_data, temp_data = train_test_split(dataset_list, test_size=0.2, random_state=42)
    val_data, test_data = train_test_split(temp_data, test_size=0.25, random_state=42)  # 15% val, 5% test

    # Convert to Hugging Face Dataset format
    dataset_dict = DatasetDict({
        "train": Dataset.from_list(train_data),
        "val": Dataset.from_list(val_data),
        "test": Dataset.from_list(test_data),
    })

    # Define Hugging Face repository name dynamically based on resolution
    repo_name = f"IDEALLab/beams_2d_{resolution.replace('x', '_')}_v0"

    # Visualize one sample optimal_design
    sample = dataset_dict['train'][0]['optimal_design']
    plt.figure(figsize=(10, 5))
    sns.heatmap(sample)
    plt.title(f"Sample from {resolution} dataset")
    plt.show()

    # Upload dataset to Hugging Face
    dataset_dict.push_to_hub(repo_name)

    print(f"Dataset for {resolution} successfully uploaded to Hugging Face at {repo_name}!")

print("All datasets successfully uploaded to Hugging Face!")

```
